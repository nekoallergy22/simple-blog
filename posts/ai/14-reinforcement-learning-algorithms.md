---
title: "強化学習のアルゴリズム"
date: "2025-6-29"
tags: ["Q学習", "Actor-Critic", "方策勾配法", "ε-greedy", "UCB"]
section: "ai"
slug: "14-reinforcement-learning-algorithms"
number: 14
category: "強化学習・評価編"
---

前回、強化学習の基礎理論について学びました。でも、実際にコンピュータに「試行錯誤して学習しなさい」と言っても、具体的にどうやって学習すればいいのでしょうか？この記事では、強化学習で使われる代表的なアルゴリズムを、身近な例を使って分かりやすく解説します。

## 探索と活用のジレンマ

強化学習のアルゴリズムを理解する前に、まず **「探索と活用のジレンマ」** について理解する必要があります。これは強化学習における最も基本的な問題の一つです。

### 探索と活用とは

**探索（Exploration）** とは、「新しい行動を試してみること」です。まだ試したことのない行動を選んで、それがどんな結果をもたらすかを調べます。

**活用（Exploitation）** とは、「これまでの経験で最も良いと分かっている行動を選ぶこと」です。確実に良い結果が得られる行動を選択します。

**たとえば**：

- **レストラン選び**：
  - 探索：新しいレストランを試してみる（もしかしたらとても美味しいかも）
  - 活用：いつものお気に入りのレストランに行く（確実に美味しい）

- **投資**：
  - 探索：新しい投資商品を試してみる（大きな利益が得られるかも）
  - 活用：これまでの実績が良い投資商品を選ぶ（安定した利益が期待できる）

要するに、「冒険して新しいことを試すか、安全な選択をするか」のバランスを取る必要があるのです。

## ε-greedy方策

**ε-greedy方策（イプシロン・グリーディー方策）** は、探索と活用のバランスを取る最もシンプルな方法です。

### 基本的な仕組み

1. **確率 ε で探索**：ランダムに行動を選ぶ
2. **確率 (1-ε) で活用**：これまでの経験で最も良い行動を選ぶ

**たとえば**：
- ε = 0.1 の場合：10%の確率で新しい行動を試し、90%の確率で最良の行動を選ぶ
- ε = 0.3 の場合：30%の確率で新しい行動を試し、70%の確率で最良の行動を選ぶ

### εの調整

学習の進行に合わせて ε を調整することが重要です：

- **学習初期（ε = 0.5〜0.9）**：まだ何も分からないので、積極的に探索する
- **学習中期（ε = 0.1〜0.3）**：ある程度分かってきたが、まだ探索も必要
- **学習後期（ε = 0.01〜0.05）**：ほぼ最適解が分かったので、たまに探索する程度

要するに、「最初は色々試してみて、だんだん良い選択に絞り込んでいく」という学習戦略です。

## UCB方策

**UCB方策（Upper Confidence Bound）** は、ε-greedy方策よりも賢い探索方法です。「不確実性の高い行動を優先的に試す」という考え方に基づいています。

### 基本的な考え方

UCB方策では、各行動に対して **「信頼区間の上限」** を計算し、それが最も高い行動を選択します。

**たとえば**：

- **行動A**：平均報酬 8点、試行回数 100回 → 信頼区間 [7.5, 8.5]
- **行動B**：平均報酬 7.5点、試行回数 10回 → 信頼区間 [6.0, 9.0]

この場合、行動Bの信頼区間の上限（9.0）が最も高いので、行動Bを選択します。

### UCBの特徴

- **試行回数の少ない行動を優先**：まだよく分からない行動を積極的に試す
- **理論的な裏付け**：数学的に最適性が証明されている
- **自動的な調整**：学習の進行に合わせて自動的に探索と活用のバランスが調整される

要するに、「よく分からない選択肢ほど、もしかしたら良いかもしれないから試してみよう」という戦略です。

## Q学習（Q-Learning）

**Q学習** は、強化学習で最も有名なアルゴリズムの一つです。前回説明した行動価値関数 Q(s,a) を学習によって更新していく手法です。

### Q学習の基本的な仕組み

Q学習では、**Qテーブル** という表を使って、各状態・行動ペアの価値を記録します。

**たとえば（迷路ゲーム）**：

```
状態＼行動    上    下    左    右
位置(1,1)   0.5   0.2   0.1   0.8
位置(1,2)   0.7   0.4   0.6   0.3
位置(2,1)   0.9   0.1   0.5   0.7
```

### Q値の更新式

Q学習では、以下の式でQ値を更新します：

```
Q(s,a) ← Q(s,a) + α × [r + γ × max Q(s',a') - Q(s,a)]
```

これを言葉で説明すると：

1. **現在のQ値**：今までの経験から学んだ価値
2. **実際の報酬**：今回の行動で得られた報酬
3. **未来の期待値**：次の状態で期待できる最大価値
4. **学習率α**：新しい経験をどれくらい重視するか

**たとえば**：
- 現在の位置(1,1)で「右」を選択
- 報酬+1を獲得して位置(1,2)に移動
- 位置(1,2)での最大Q値は0.7
- 学習率α=0.1、割引率γ=0.9として更新

要するに、「期待していた価値と実際の価値の差を使って、徐々に正確な価値を学習していく」方法です。

### Q学習の特徴

- **モデルフリー**：環境の詳細な仕組みを知らなくても学習できる
- **オフポリシー**：実際に取った行動と、学習に使う行動が違っても良い
- **収束保証**：適切な条件下で最適解に収束することが数学的に証明されている

## Actor-Critic法

**Actor-Critic法** は、「行動を選ぶ部分」と「価値を評価する部分」を分離した強化学習アルゴリズムです。

### Actor-Criticの構成

**Actor（俳優）**：
- 方策を学習する部分
- 「この状況ではこの行動を取るべき」という判断を行う
- 要するに、「実際に行動を決める人」

**Critic（批評家）**：
- 価値関数を学習する部分
- 「この状況は良い状況か悪い状況か」という評価を行う
- 要するに、「行動の良し悪しを評価する人」

### 学習の流れ

1. **Actor** が現在の状況で行動を選択
2. **Critic** がその行動と結果を評価
3. **Critic** の評価に基づいて **Actor** が行動戦略を修正
4. **Critic** も実際の結果と自分の評価の差から学習

**たとえば**：

- **ゲームの場面**：
  - Actor：「この場面では右に動こう」
  - Critic：「その判断は良くない、左の方が良い状況になりそう」
  - Actor：「次からは左を選ぶ確率を上げよう」
  - Critic：「実際の結果を見て、自分の評価も修正しよう」

要するに、「行動する人と評価する人が協力して、お互いに学習していく」システムです。

## REINFORCE

**REINFORCE** は、**方策勾配法** の代表的なアルゴリズムです。Q学習が価値関数を学習するのに対し、REINFORCEは方策（行動を選ぶ確率）を直接学習します。

### 方策勾配法の基本的な考え方

従来の方法では「どの行動が良いか」を学習していましたが、方策勾配法では「各行動を選ぶ確率」を直接学習します。

**たとえば**：

- **従来の方法**：「右の価値は0.8、左の価値は0.3」
- **方策勾配法**：「右を選ぶ確率は70%、左を選ぶ確率は30%」

### REINFORCEの特徴

- **連続的な行動空間**：「30度右に曲がる」のような連続的な行動も扱える
- **確率的方策**：状況に応じて行動を確率的に選択できる
- **方策の直接最適化**：価値関数を経由せずに、直接最適な方策を学習

### 学習の仕組み

1. **現在の方策**で行動を選択
2. **一連の行動**を実行してエピソードを完了
3. **得られた報酬**に基づいて方策を更新
4. **良い結果**をもたらした行動の確率を上げ、**悪い結果**をもたらした行動の確率を下げる

要するに、「実際にやってみて、うまくいった行動パターンを繰り返しやすくし、うまくいかなかった行動パターンを避けるようにする」方法です。

## アルゴリズムの使い分け

### 問題の特性による選択

**離散的な行動空間**：
- Q学習：シンプルで理解しやすい
- Actor-Critic：複雑だが高性能

**連続的な行動空間**：
- REINFORCE：シンプルだが学習が不安定
- Actor-Critic：安定した学習が可能

### 環境による選択

**シンプルな環境**：
- ε-greedy + Q学習：実装が簡単で十分な性能

**複雑な環境**：
- UCB方策 + Actor-Critic：高度な探索と学習が可能

要するに、問題の複雑さや要求される性能に応じて、適切なアルゴリズムを選択することが重要です。

## まとめ

強化学習のアルゴリズムは、探索と活用のバランスを取りながら、価値関数や方策を学習することで最適な行動を見つけ出します。Q学習は価値ベース、REINFORCEは方策ベース、Actor-Criticはその両方を組み合わせた手法です。

要するに、強化学習は「試行錯誤の戦略」と「学習の仕組み」を組み合わせることで、正解のない問題でも最適解を見つけ出せる技術なのです。

次回は、これらのアルゴリズムがどれくらい良い性能を発揮しているかを測るための評価指標について解説していきます。