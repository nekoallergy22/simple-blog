---
title: "活性化関数の種類と特徴"
date: "2025-6-29"
tags: ["活性化関数", "ReLU", "シグモイド", "tanh", "勾配消失問題", "ニューラルネットワーク"]
section: "ai"
slug: "19-activation-functions"
number: 19
category: "強化学習・評価編"
---

ニューラルネットワークが複雑な問題を解けるのは、**活性化関数** という小さな「スイッチ」のおかげです。この記事では、活性化関数がなぜ必要なのか、どんな種類があるのか、そしてそれぞれの特徴について、身近な例を使って分かりやすく解説します。

## 活性化関数とは何か？

**活性化関数とは、ニューロンが「発火するかどうか」を決める関数** です。人間の神経細胞が一定の刺激を受けると電気信号を発するように、人工ニューロンも一定の条件で信号を出力します。

### なぜ活性化関数が必要なのか？

活性化関数がないと、ニューラルネットワークは **単なる線形関数** になってしまいます。

**たとえば**、活性化関数なしの2層ネットワークを考えてみましょう：

```
入力 → 層1（y = ax + b） → 層2（z = cy + d） → 出力
```

これを展開すると：
```
z = c(ax + b) + d = (ca)x + (cb + d)
```

要するに、何層重ねても **1つの直線** と同じ表現力しかないのです。

しかし、活性化関数があると：
- **非線形な関係** を表現できる
- **複雑な境界** を学習できる
- **階層的な特徴抽出** が可能になる

**たとえば**、XOR問題（「AまたはBの片方だけが真の時に真」）は、線形関数では解けませんが、非線形な活性化関数があれば解けるようになります。

## 代表的な活性化関数

### 1. ステップ関数：最もシンプルな活性化関数

**ステップ関数** は、最も基本的な活性化関数です：

```
f(x) = 1 (x ≥ 0の時)
f(x) = 0 (x < 0の時)
```

**たとえば**、電気のスイッチのように「オン」か「オフ」かを決めます。

**特徴**：
- ✅ **理解しやすい**：人間の神経細胞に近い動作
- ❌ **微分不可能**：学習アルゴリズムで使えない
- ❌ **情報損失**：入力の大きさの情報が失われる

現在はほとんど使われていませんが、パーセプトロンの歴史を理解するために重要です。

### 2. シグモイド関数：滑らかなS字カーブ

**シグモイド関数** は、ステップ関数を滑らかにしたものです：

```
f(x) = 1 / (1 + e^(-x))
```

グラフは滑らかなS字カーブを描き、出力は0から1の間の値になります。

**たとえば**、「試験の点数から合格確率を計算する」ような場面で使われます：
- 0点 → 合格確率 ほぼ0%
- 50点 → 合格確率 50%
- 100点 → 合格確率 ほぼ100%

**特徴**：
- ✅ **微分可能**：勾配降下法で学習できる
- ✅ **確率的解釈**：出力を確率として解釈可能
- ❌ **勾配消失問題**：深いネットワークで学習困難
- ❌ **計算コスト**：指数関数の計算が重い

### 3. tanh関数：改良されたシグモイド

**tanh関数（双曲線正接関数）** は、シグモイド関数の改良版です：

```
f(x) = tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

出力は-1から1の間の値になります。

**シグモイドとの違い**：
- **シグモイド**：出力範囲 0～1
- **tanh**：出力範囲 -1～1（ゼロ中心）

**たとえば**、株価の変動予測で使うなら：
- +1 → 大幅上昇
- 0 → 変化なし
- -1 → 大幅下落

**特徴**：
- ✅ **ゼロ中心**：学習が安定しやすい
- ✅ **シグモイドより勾配が大きい**：学習効率が良い
- ❌ **依然として勾配消失問題**：深いネットワークでは限界

### 4. ReLU関数：現代の主流

**ReLU（Rectified Linear Unit）関数** は、現在最も広く使われている活性化関数です：

```
f(x) = max(0, x)
```

要するに、「負の値は0に、正の値はそのまま」という非常にシンプルな関数です。

**たとえば**、音量調整のようなものです：
- マイナスの音量（意味不明） → 0（無音）
- プラスの音量 → そのままの音量

**特徴**：
- ✅ **計算が高速**：max演算だけで済む
- ✅ **勾配消失問題の軽減**：正の領域で勾配が1
- ✅ **スパース性**：負の値が0になることで効率的
- ❌ **Dead ReLU問題**：負の領域で完全に停止

### 5. Leaky ReLU：ReLUの改良版

**Leaky ReLU** は、ReLUのDead ReLU問題を解決するために開発されました：

```
f(x) = max(0.01x, x)
```

負の値でも小さな勾配（通常0.01）を保持します。

**たとえば**、音量調整で「完全に無音にするのではなく、とても小さな音量は残しておく」ようなイメージです。

**特徴**：
- ✅ **Dead ReLU問題の解決**：負の領域でも学習継続
- ✅ **ReLUより安定**：すべての領域で勾配あり
- ❌ **パラメータ調整**：傾きの値を決める必要

## 勾配消失問題とは？

**勾配消失問題** は、深いニューラルネットワークで発生する深刻な問題です。

### 問題の仕組み

学習時、誤差は出力層から入力層に向かって **逆方向** に伝播します。このとき、各層で勾配（微分値）が掛け算されていきます。

**たとえば**、5層のネットワークでシグモイド関数を使った場合：
```
最終層の勾配 = 0.25 × 0.25 × 0.25 × 0.25 × 0.25 = 0.001
```

**シグモイド関数の最大勾配は0.25** なので、層を重ねるほど勾配が **指数関数的に小さく** なります。

### 実際の影響

勾配消失問題が起きると：
- **初期の層が学習しない**：勾配がほぼ0になる
- **学習が非常に遅い**：重みの更新量が極小
- **表現力の低下**：深いネットワークの利点が失われる

**たとえば**、10人でメッセージを伝言ゲームするとき、各人が50%の確率で内容を忘れるとすると、最後まで正確に伝わる確率は約0.1%しかありません。

### ReLUによる解決

**ReLU関数** は勾配消失問題を大幅に軽減します：

- **正の領域での勾配は1**：勾配が減衰しない
- **負の領域での勾配は0**：完全に停止するが消失しない
- **スパース性**：重要でない情報をカットアウト

要するに、ReLUは「必要な情報は完全に伝え、不要な情報は完全にカットする」明確な特性を持っています。

## 活性化関数の選び方

### タスク別の選択指針

**分類問題（出力層）**：
- **二分類**：シグモイド関数（確率0～1）
- **多クラス分類**：ソフトマックス関数（確率分布）

**回帰問題（出力層）**：
- **正の値のみ**：ReLU
- **任意の実数**：線形関数（活性化関数なし）

**隠れ層**：
- **一般的な用途**：ReLU（デフォルト選択）
- **勾配消失が深刻**：Leaky ReLU
- **レガシーシステム**：tanh

### 実践的なアドバイス

**初心者向け**：
1. **隠れ層は ReLU** を使う
2. **出力層はタスクに応じて** 選択
3. **問題があれば Leaky ReLU** を試す

**上級者向け**：
- **ELU**：指数関数的な負の値
- **Swish**：Google開発の新しい関数
- **GELU**：Transformer で人気

## 現代の活性化関数の発展

### Swish関数

**Swish関数** は、Googleが開発した比較的新しい活性化関数です：

```
f(x) = x × sigmoid(x)
```

**特徴**：
- **滑らかな曲線**：すべての点で微分可能
- **自己ゲート化**：入力値に応じて自動調整
- **優秀な性能**：多くのタスクでReLUより高精度

### GELU関数

**GELU（Gaussian Error Linear Unit）** は、特に **Transformer** モデルで人気の活性化関数です：

```
f(x) = x × P(X ≤ x)  # Xは標準正規分布
```

**特徴**：
- **確率的解釈**：正規分布に基づく設計
- **優秀な汎化性能**：過学習しにくい
- **Transformer で標準**：BERT、GPT などで使用

要するに、活性化関数の研究は現在も活発で、新しい関数が継続的に提案されています。

## まとめ

活性化関数は、ニューラルネットワークに「非線形性」を与える重要な要素です。要するに、「AIが複雑な問題を解けるかどうか」は、この小さな関数の選択にかかっているのです。

重要なポイントは：
- **ReLU** が現在の主流（シンプルで効果的）
- **勾配消失問題** を理解して適切な関数を選択
- **タスクに応じた出力層** の活性化関数選択

次回は、多クラス分類で重要な **ソフトマックス関数** について、確率分布としての特性と出力層での役割を詳しく解説していきます。