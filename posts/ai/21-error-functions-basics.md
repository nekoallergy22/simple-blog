---
title: "誤差関数の基礎"
date: "2025-6-29"
tags: ["誤差関数", "損失関数", "交差エントロピー", "平均二乗誤差", "回帰", "分類"]
section: "ai"
slug: "21-error-functions-basics"
number: 21
category: "強化学習・評価編"
---

ニューラルネットワークが「学習する」とは、具体的には **「間違いを測り、それを小さくしていく」** ことです。この「間違いの大きさ」を測る物差しが **誤差関数（損失関数）** です。この記事では、基本的な誤差関数の種類と、回帰・分類問題での使い分けについて分かりやすく解説します。

## 誤差関数ってなに？

**誤差関数（Loss Function / Cost Function）とは、AIの予測と正解の「ズレの大きさ」を数値で表す関数** です。人間が試験で採点するように、AIも自分の答えがどれだけ間違っているかを知る必要があります。

### 学習プロセスでの役割

誤差関数は以下のような役割を果たします：

1. **現在の性能を測定**：「どれだけ間違っているか」の定量化
2. **学習の方向を決定**：「どちらに修正すべきか」の指標
3. **学習の終了判定**：「十分に学習できたか」の基準

**たとえば**、車の運転を学ぶとき：
- **予測**：「右に3度ハンドルを切る」
- **正解**：「右に5度ハンドルを切るべきだった」
- **誤差**：2度の差（修正すべき量）

要するに、誤差関数は「AIの通信簿」のような役割を果たしているのです。

## 回帰問題の誤差関数

### 平均二乗誤差（MSE：Mean Squared Error）

**平均二乗誤差** は、回帰問題で最も基本的な誤差関数です：

```
MSE = (1/n) × Σ(予測値 - 正解値)²
```

**たとえば**、家の価格予測で以下の結果が出たとします：

| 実際の価格 | 予測価格 | 誤差 | 誤差² |
|----------|---------|------|-------|
| 3000万円 | 2800万円 | 200万円 | 40000 |
| 4000万円 | 4200万円 | -200万円 | 40000 |
| 5000万円 | 5100万円 | -100万円 | 10000 |

MSE = (40000 + 40000 + 10000) / 3 = 30000

### なぜ「二乗」するのか？

誤差を二乗する理由は3つあります：

1. **正負を統一**：+200万円の誤差も-200万円の誤差も同じ「間違い」として扱う
2. **大きな誤差を強調**：200万円の誤差は100万円の誤差の4倍重要とみなす
3. **微分可能性**：学習アルゴリズムで使いやすい数学的性質

**たとえば**、射的で：
- 中心から1cm外れる：誤差1
- 中心から2cm外れる：誤差4（2倍外れただけだが、4倍重要）

要するに、「大きなミス」をより強く反省する仕組みなのです。

### 平均絶対誤差（MAE：Mean Absolute Error）

**平均絶対誤差** は、誤差の絶対値を使う関数です：

```
MAE = (1/n) × Σ|予測値 - 正解値|
```

先ほどの家の価格予測例では：
MAE = (200 + 200 + 100) / 3 = 166.7万円

**MSE vs MAE の特徴**：

| 特徴 | MSE | MAE |
|------|-----|-----|
| 外れ値への感度 | 高い（二乗効果） | 低い（線形効果） |
| 計算の複雑さ | やや複雑 | シンプル |
| 微分の性質 | 滑らか | 角がある |
| 実用的解釈 | やや難しい | 直感的 |

**たとえば**、株価予測で：
- **MSE**: 大きく外れた予測を厳しく評価（リスク重視）
- **MAE**: すべての誤差を平等に評価（平均的性能重視）

## 分類問題の誤差関数

### 交差エントロピー（Cross Entropy）

**交差エントロピー** は、分類問題で最も広く使われる誤差関数です。**「予測した確率分布と正解の確率分布の違い」** を測ります。

#### 二分類の場合（Binary Cross Entropy）

```
BCE = -(1/n) × Σ[y×log(p) + (1-y)×log(1-p)]
```

**たとえば**、メールのスパム判定で：

| メール | 正解 | 予測確率 | 計算 |
|--------|------|----------|------|
| A | スパム(1) | 0.9 | -1×log(0.9) = 0.11 |
| B | 正常(0) | 0.2 | -0×log(0.2) - 1×log(0.8) = 0.22 |
| C | スパム(1) | 0.6 | -1×log(0.6) = 0.51 |

BCE = (0.11 + 0.22 + 0.51) / 3 = 0.28

#### 多クラス分類の場合（Categorical Cross Entropy）

```
CCE = -(1/n) × ΣΣ y_ij × log(p_ij)
```

**たとえば**、動物の分類（犬、猫、鳥）で：

正解: 犬 [1, 0, 0]
予測: [0.7, 0.2, 0.1]
誤差: -1×log(0.7) - 0×log(0.2) - 0×log(0.1) = 0.36

### なぜ対数（log）を使うのか？

対数を使う理由は以下の通りです：

1. **確率の性質に適合**：確率0に近づくと誤差が急激に増加
2. **情報理論との対応**：情報量の概念と一致
3. **数値的安定性**：勾配計算が安定
4. **最尤推定との対応**：統計学的に適切

**たとえば**、天気予報で：
- 「雨の確率90%」で実際に雨 → 小さな誤差
- 「雨の確率10%」で実際に雨 → 大きな誤差

要するに、「確信を持って間違えた」場合により大きなペナルティを与える仕組みです。

## 誤差関数の選び方

### 問題の種類による選択

**回帰問題**：
- **一般的な場合**：MSE（平均二乗誤差）
- **外れ値が多い**：MAE（平均絶対誤差）
- **より複雑な評価**：Huber損失、Quantile損失

**分類問題**：
- **二分類**：Binary Cross Entropy
- **多クラス分類**：Categorical Cross Entropy
- **不均衡データ**：Weighted Cross Entropy、Focal Loss

### 実践的な判断基準

**MSE を選ぶべき場合**：
- 大きな誤差を特に避けたい
- データに外れ値が少ない
- 予測値の分散も重要

**MAE を選ぶべき場合**：
- 外れ値に頑健な予測が欲しい
- 誤差の解釈を簡単にしたい
- 平均的な性能を重視

**Cross Entropyを選ぶべき場合**：
- 分類問題（これが標準）
- 確率的な解釈が重要
- ソフトマックス関数と組み合わせる

## 誤差関数の特徴と注意点

### 1. スケールの影響

**MSE** は出力値のスケールに敏感です。

**たとえば**：
- 価格予測（単位：円）：誤差が数万～数百万
- 身長予測（単位：m）：誤差が0.01～0.1

同じモデルでも、単位が違うだけで誤差の大きさが全く異なります。

### 2. 最適化のしやすさ

**微分可能性** は学習の効率に大きく影響します：

- **MSE, Cross Entropy**: 滑らかで微分しやすい
- **MAE**: 0で微分不可能（実装上の工夫が必要）

### 3. 解釈のしやすさ

**直感的理解** も重要な要素です：

- **MAE**: 「平均的にX円間違っている」
- **MSE**: 「二乗平均でY円²間違っている」（解釈しにくい）
- **Cross Entropy**: 「情報理論的にZ bit間違っている」（専門的）

## 実装例と計算効率

### NumPy での実装例

```python
import numpy as np

def mse(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

def binary_cross_entropy(y_true, y_pred):
    # 数値的安定性のためのクリッピング
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    return -np.mean(y_true * np.log(y_pred) + 
                    (1 - y_true) * np.log(1 - y_pred))
```

### 計算効率の考慮

**大規模データセット** では計算効率も重要です：

- **ベクトル化**: NumPy/TensorFlowの最適化を活用
- **バッチ処理**: メモリ使用量の管理
- **並列計算**: GPU/TPUでの高速化

## まとめ

誤差関数は、AIが「どれだけ上達したか」を測る重要な道具です。要するに、**「間違いの測り方」を正しく選ぶことで、AIが正しい方向に学習できる** のです。

重要なポイントは：
- **回帰問題**: MSE（標準）、MAE（頑健性重視）
- **分類問題**: Cross Entropy（ほぼ唯一の選択）
- **問題の特性** に応じた適切な選択

次回は、より特殊なタスク向けの **高度な誤差関数**（Contrastive Loss、Triplet Loss、KL散度など）について、具体的な応用例と共に詳しく解説していきます。