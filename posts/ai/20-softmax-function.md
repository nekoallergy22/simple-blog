---
title: "ソフトマックス関数と出力層"
date: "2025-6-29"
tags: ["ソフトマックス", "多クラス分類", "確率分布", "出力層", "活性化関数"]
section: "ai"
slug: "20-softmax-function"
number: 20
category: "強化学習・評価編"
---

スマートフォンのカメラが写真を見て「これは犬です（85%の確信）」「猫かもしれません（12%の確信）」と判断できるのは、**ソフトマックス関数** のおかげです。この記事では、多クラス分類で重要な役割を果たすソフトマックス関数について、確率の観点から分かりやすく解説します。

## ソフトマックス関数とは何か？

**ソフトマックス関数とは、複数の数値を「確率分布」に変換する関数** です。たとえば、ニューラルネットワークが「3.2, 1.8, 0.5」という数値を出力したとき、これを「犬70%、猫25%、鳥5%」という確率に変換します。

### 基本的な仕組み

ソフトマックス関数は以下の式で定義されます：

```
softmax(xi) = e^xi / Σ(e^xj)
```

**たとえば**、3つのクラス（犬、猫、鳥）で次の値が出力されたとします：
- 犬: 2.0
- 猫: 1.0  
- 鳥: 0.0

ソフトマックス変換の手順：

1. **指数関数を適用**：
   - 犬: e^2.0 ≈ 7.39
   - 猫: e^1.0 ≈ 2.72
   - 鳥: e^0.0 = 1.00

2. **合計を計算**：7.39 + 2.72 + 1.00 = 11.11

3. **各値を合計で割る**：
   - 犬: 7.39/11.11 ≈ 0.665 (66.5%)
   - 猫: 2.72/11.11 ≈ 0.245 (24.5%)
   - 鳥: 1.00/11.11 ≈ 0.090 (9.0%)

要するに、「生の数値」を「確率として解釈できる数値」に変換する関数なのです。

## なぜ指数関数を使うのか？

### 1. 正の値を保証

確率は **必ず正の値** でなければなりません。指数関数 e^x は、どんな実数 x に対しても必ず正の値を返します。

**たとえば**：
- e^(-100) ≈ 0.0000...（限りなく0に近いが正の値）
- e^0 = 1
- e^100 ≈ 非常に大きな正の値

### 2. 差を拡大する効果

指数関数は **小さな差を大きく拡大** します。これにより、「少し優勢な選択肢」を「明確に優勢な選択肢」に変換できます。

**たとえば**、2つの候補の差が0.5だった場合：
- **線形変換**：2.5 vs 2.0 → 差は0.5
- **指数変換**：e^2.5 ≈ 12.18 vs e^2.0 ≈ 7.39 → 差は4.79

要するに、「曖昧な判断」を「明確な判断」に変換する効果があります。

### 3. 微分可能性

ソフトマックス関数は **すべての点で微分可能** なので、勾配降下法による学習が可能です。これは機械学習において重要な特性です。

## ソフトマックス関数の特徴

### 1. 確率分布の性質

ソフトマックス関数の出力は **完全な確率分布** になります：

- **すべて正の値**：0 ≤ softmax(xi) ≤ 1
- **合計が1**：Σ softmax(xi) = 1
- **相対的な大小関係保持**：xi > xj なら softmax(xi) > softmax(xj)

**たとえば**、天気予報で「晴れ60%、曇り30%、雨10%」と言えるのも、これらが確率分布の条件を満たしているからです。

### 2. 温度パラメータ

ソフトマックス関数には **温度パラメータ T** を導入できます：

```
softmax(xi) = e^(xi/T) / Σ(e^(xj/T))
```

**温度の効果**：
- **T = 1**：標準的なソフトマックス
- **T → 0**：最大値に近い要素のみが1に近づく（鋭い分布）
- **T → ∞**：すべての要素が均等に近づく（平坦な分布）

**たとえば**、AIの創造性を調整する場面で使われます：
- **低温度**：「最も確実な答えだけを選ぶ」（保守的）
- **高温度**：「様々な可能性を考慮する」（創造的）

### 3. 計算の安定性

素直にソフトマックスを計算すると、**数値オーバーフロー** が起きる可能性があります。

**たとえば**、入力が [1000, 999, 998] の場合：
- e^1000 は計算不可能な巨大数
- コンピュータが「無限大」エラーを起こす

**解決策**：最大値を引いて計算します
```
softmax(xi) = e^(xi-max) / Σ(e^(xj-max))
```

要するに、相対的な関係は変わらないので、計算しやすい範囲に数値を調整するのです。

## 多クラス分類での活用

### 出力層での役割

**多クラス分類問題** では、出力層にソフトマックス関数を適用するのが標準的です。

**たとえば**、手書き数字認識（0-9の10クラス）では：

```
隠れ層 → 出力層（10個のニューロン） → ソフトマックス → 確率分布
[...] → [2.1, -1.0, 0.5, ...] → [0.65, 0.03, 0.12, ...]
```

各出力が「その数字である確率」を表します。

### One-Hot エンコーディングとの対応

**教師データ** は通常、One-Hot エンコーディングで表現されます：

**たとえば**、数字「3」の場合：
```
教師データ: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
予測結果:   [0.1, 0.05, 0.1, 0.7, 0.02, 0.01, 0.01, 0.01, 0.0, 0.0]
```

ソフトマックスの出力と教師データを比較することで、学習が進みます。

### 予測の解釈

ソフトマックス関数の出力は **AIの確信度** を表します：

**高い確信度**：
```
[0.95, 0.02, 0.01, 0.01, 0.01, ...]
→ 「95%の確信で犬だと思います」
```

**低い確信度**：
```
[0.35, 0.25, 0.15, 0.12, 0.08, ...]
→ 「よく分からないけど、犬かもしれません」
```

要するに、AIが「自分の判断にどれだけ自信があるか」が数値で分かるのです。

## 他の活性化関数との比較

### シグモイド関数との違い

**シグモイド関数**：
- **用途**：二分類（Yes/No の判断）
- **出力範囲**：0～1（1つの値）
- **解釈**：「Yesである確率」

**ソフトマックス関数**：
- **用途**：多クラス分類（複数選択肢から1つ）
- **出力範囲**：各要素0～1、合計1（複数の値）
- **解釈**：「各選択肢の確率分布」

**たとえば**：
- **シグモイド**：「この写真に犬が写っている確率は85%」
- **ソフトマックス**：「犬70%、猫20%、鳥10%」

### 線形関数（活性化関数なし）との違い

**線形関数**：
- **出力**：任意の実数値
- **解釈**：数値的な大小関係のみ

**ソフトマックス関数**：
- **出力**：確率分布
- **解釈**：各選択肢の可能性

**たとえば**、学生の成績評価で：
- **線形**：「数学95点、英語78点、理科82点」
- **ソフトマックス**：「数学が得意62%、英語が得意18%、理科が得意20%」

## 実装上の注意点

### 1. 数値的安定性

```python
def stable_softmax(x):
    # 最大値を引いて数値オーバーフローを防ぐ
    x_max = np.max(x)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x)
```

### 2. ベクトル化計算

大量のデータを効率的に処理するために、**ベクトル化** された実装が重要です。

### 3. 勾配計算

ソフトマックス関数の勾配は特殊な形になるため、フレームワーク（TensorFlow、PyTorch）の実装を使うのが安全です。

## 実世界での応用例

### 1. 画像認識

**ImageNet分類**：1000種類の物体を分類
```
入力: 写真
出力: [犬0.85, 猫0.12, 鳥0.02, ...]
```

### 2. 自然言語処理

**機械翻訳**：次の単語を予測
```
入力: "I love"
出力: [you0.4, cats0.3, music0.2, ...]
```

### 3. ゲームAI

**囲碁・将棋**：次の手を選択
```
入力: 現在の盤面
出力: [位置A0.35, 位置B0.28, 位置C0.15, ...]
```

## まとめ

ソフトマックス関数は、AIが「複数の選択肢から1つを選ぶ」ときに必要不可欠な技術です。要するに、「コンピュータの数値計算」を「人間が理解できる確率」に変換する **翻訳機** のような存在なのです。

重要なポイントは：
- **確率分布** への変換（合計が1、すべて正の値）
- **多クラス分類** での標準的な選択
- **数値的安定性** を考慮した実装の重要性

次回は、ニューラルネットワークの学習で使われる様々な **誤差関数** について、回帰問題と分類問題での使い分けを詳しく解説していきます。