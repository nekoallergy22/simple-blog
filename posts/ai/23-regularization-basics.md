---
title: "正則化の基本技術"
date: "2025-6-29"
tags: ["正則化", "L1正則化", "L2正則化", "ラッソ回帰", "リッジ回帰", "過学習対策"]
section: "ai"
slug: "23-regularization-basics"
number: 23
category: "強化学習・評価編"
---

AIモデルが訓練データでは100点なのに、新しいデータでは30点しか取れない。これは **過学習** という問題です。この記事では、過学習を防ぐ重要な技術である **正則化** について、L1・L2正則化の仕組みから回帰問題での実践的応用まで、分かりやすく解説します。

## 正則化ってなに？

**正則化（Regularization）とは、モデルが複雑になりすぎることを防ぐ技術** です。人間に例えると、「教科書の内容を丸暗記するのではなく、本質を理解して応用力を身につける」ような学習法と言えます。

### 過学習の問題

**過学習** は、モデルが訓練データの細かいパターンまで覚えすぎてしまう現象です。

**たとえば**、学生の成績予測で：

| 学習時間 | 睡眠時間 | 成績 |
|----------|----------|------|
| 2時間 | 6時間 | 60点 |
| 4時間 | 7時間 | 80点 |
| 6時間 | 5時間 | 70点 |

**過学習したモデル**: 「学習時間が4.0時間、睡眠時間が7.0時間なら必ず80点」と覚える
**良いモデル**: 「学習時間が多く、適度な睡眠を取ると成績が良い」と一般化する

過学習したモデルは、訓練データと少しでも違う条件（学習4.1時間、睡眠6.9時間）では全く予測できなくなります。

### 正則化の基本概念

正則化は **「シンプルなモデルほど良い」** という哲学に基づいています。これを **オッカムの剃刀** の原理と呼びます。

**たとえば**、株価予測で：
- **複雑なモデル**: 100個の経済指標を使って予測
- **シンプルなモデル**: 3個の重要な指標だけで予測

シンプルなモデルの方が、新しい状況でも安定して動作することが多いのです。

## L0・L1・L2正則化の比較

### L0正則化：使用する特徴数を制限

**L0正則化** は、実際に使用する特徴（変数）の数を制限します。

```
L0正則化項 = λ × (使用する特徴数)
```

**たとえば**、不動産価格予測で：
- 候補：面積、駅距離、築年数、階数、方角...（50個の特徴）
- L0正則化：そのうち5個だけを選ぶ

**特徴**：
- ✅ **解釈しやすい**: 重要な特徴だけが残る
- ✅ **計算効率**: 使用特徴数が少ない
- ❌ **計算困難**: 組み合わせ最適化問題（NP困難）

### L1正則化（Lasso）：重みを0に追い込む

**L1正則化** は、重みの絶対値の和をペナルティとして加えます。

```
総コスト = 通常の誤差 + λ × Σ|wi|
```

**特徴**：
- **自動特徴選択**: 重要でない特徴の重みを **正確に0** にする
- **スパースモデル**: 多くの重みが0になる
- **解釈性**: どの特徴が重要かが明確

**たとえば**、健康診断で病気リスクを予測する場合：

| 特徴 | L1正則化前の重み | L1正則化後の重み |
|------|------------------|------------------|
| 年齢 | 0.8 | 0.6 |
| BMI | 0.6 | 0.4 |
| 血圧 | 0.3 | 0.2 |
| 趣味（映画鑑賞） | 0.02 | **0.0** |
| 好きな色 | -0.01 | **0.0** |

要するに、「病気リスクに無関係な特徴」の重みを **完全に0** にして、重要な特徴だけを残すのです。

### L2正則化（Ridge）：重みを小さく抑制

**L2正則化** は、重みの二乗和をペナルティとして加えます。

```
総コスト = 通常の誤差 + λ × Σwi²
```

**特徴**：
- **重みの抑制**: すべての重みを小さな値に抑える
- **安定性**: 外れ値に対して頑健
- **非スパース**: 重みが0になることは稀

**たとえば**、同じ健康診断の例で：

| 特徴 | L2正則化前の重み | L2正則化後の重み |
|------|------------------|------------------|
| 年齢 | 0.8 | 0.5 |
| BMI | 0.6 | 0.4 |
| 血圧 | 0.3 | 0.2 |
| 趣味（映画鑑賞） | 0.02 | 0.01 |
| 好きな色 | -0.01 | -0.005 |

L2正則化は重みを0にするのではなく、**すべての重みを小さく** します。

## 正則化の幾何学的解釈

### L1正則化の形状

L1正則化の制約条件 `Σ|wi| ≤ C` は、**菱形（ダイヤモンド）** の形になります。

```
     w2
     |
  \  |  /
   \ | /
----+----w1
   / | \
  /  |  \
     |
```

この菱形の角（頂点）で最適解が見つかることが多く、角では1つの重みが **正確に0** になります。

### L2正則化の形状

L2正則化の制約条件 `Σwi² ≤ C` は、**円形** になります。

```
     w2
     |
    /|\
   / | \
  |  |  |
----+----w1
  |  |  |
   \ | /
    \|/
     |
```

円は滑らかなので、重みが **正確に0** になることは稀です。

### 実際の最適化での違い

**たとえば**、2つの特徴（身長・体重）で体脂肪率を予測する場合：

**L1正則化**: 「身長」と「体重」のうち、より重要な1つだけを選ぶ傾向
**L2正則化**: 「身長」と「体重」の両方を使うが、どちらも控えめな重みにする

要するに、L1は **「選択」**、L2は **「バランス」** を重視するのです。

## 回帰問題での応用

### リッジ回帰（Ridge Regression）

**リッジ回帰** は、線形回帰にL2正則化を加えた手法です。

```
コスト = MSE + λ × Σwi²
```

**応用例**：
- **多重共線性** がある場合（特徴同士が強く相関）
- **特徴数がサンプル数より多い** 場合
- **安定した予測** が必要な場合

**たとえば**、住宅価格予測で「面積」と「部屋数」が強く相関している場合、リッジ回帰は両方の重みを適度に小さくして、安定した予測を実現します。

### ラッソ回帰（Lasso Regression）

**ラッソ回帰** は、線形回帰にL1正則化を加えた手法です。

```
コスト = MSE + λ × Σ|wi|
```

**応用例**：
- **特徴選択** が重要な場合
- **解釈性** を重視する場合
- **高次元データ** の処理

**たとえば**、遺伝子データから病気を予測する場合、数万個の遺伝子から重要な10個だけを自動選択できます。

### ElasticNet：L1とL2の組み合わせ

**ElasticNet** は、L1とL2正則化を同時に使う手法です。

```
コスト = MSE + λ1 × Σ|wi| + λ2 × Σwi²
```

**利点**：
- L1の **特徴選択** 能力
- L2の **安定性**
- **グループ選択**: 相関の高い特徴をまとめて選択/除外

**たとえば**、マーケティングデータで：
- 「年収」「職業」「学歴」（相関が高い）→ まとめて選択
- 「好きな色」（無関係） → 除外

## λ（正則化パラメータ）の選び方

### λの意味と効果

**λ（ラムダ）** は、正則化の強さを制御するパラメータです。

- **λ = 0**: 正則化なし（過学習のリスク）
- **λ 小**: 軽い正則化（複雑なモデル）
- **λ 大**: 強い正則化（シンプルなモデル）
- **λ → ∞**: すべての重みが0（学習しない）

### 最適なλの選択方法

**1. 交差検証（Cross Validation）**：
データを訓練用・検証用に分けて、検証誤差が最小になるλを選択

**2. 情報量規準（AIC/BIC）**：
モデルの複雑さと精度のバランスを数値で評価

**3. Learning Curve**：
λを変化させながら訓練誤差と検証誤差をプロット

**たとえば**、λの選択プロセス：

| λ | 訓練誤差 | 検証誤差 | 判定 |
|---|----------|----------|------|
| 0.001 | 0.1 | 0.8 | 過学習 |
| 0.01 | 0.2 | 0.3 | **最適** |
| 0.1 | 0.5 | 0.6 | 学習不足 |

## 実装例とフレームワーク

### Scikit-learnでの実装

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import cross_val_score

# リッジ回帰
ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)

# ラッソ回帰
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# ElasticNet
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)
```

### パラメータ選択の自動化

```python
from sklearn.model_selection import GridSearchCV

# 複数のλ値を試して最適値を選択
parameters = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}
ridge_cv = GridSearchCV(Ridge(), parameters, cv=5)
ridge_cv.fit(X_train, y_train)
```

## まとめ

正則化は、AIモデルに **「謙虚さ」** を教える技術です。要するに、「すべてを覚えようとせず、本質的なパターンだけを学ぶ」ことで、新しい状況でも応用が利く **賢いモデル** を作ることができるのです。

重要なポイントは：
- **L1正則化**: 特徴選択・解釈性重視（ラッソ回帰）
- **L2正則化**: 安定性・バランス重視（リッジ回帰）
- **λの選択**: 交差検証で過学習と学習不足のバランスを取る

次回は、ニューラルネットワーク特有の正則化手法である **ドロップアウト** について、その仕組みと構造的正則化の効果を詳しく解説していきます。