---
title: "MLOpsãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè·µ"
date: "2024-01-18"
category: "ai-course"
slug: "18-mlops-model-deployment"
difficulty: "intermediate"
number: 18
---

# MLOpsãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè·µ

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã§å®‰å®šé‹ç”¨ã™ã‚‹ãŸã‚ã®MLOpsï¼ˆMachine Learning Operationsï¼‰ã«ã¤ã„ã¦å®Ÿè·µçš„ã«å­¦ç¿’ã—ã¾ã™ã€‚

## MLOpsã¨ã¯

**MLOps**ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‹ã‚‰æœ¬ç•ªé‹ç”¨ã¾ã§å…¨å·¥ç¨‹ã‚’åŠ¹ç‡åŒ–ãƒ»è‡ªå‹•åŒ–ã™ã‚‹æ‰‹æ³•ãƒ»æ–‡åŒ–ãƒ»æŠ€è¡“ã®ç·ç§°ã§ã™ã€‚

### å¾“æ¥ã®é–‹ç™º vs MLOps
| å¾“æ¥ | MLOps |
|------|-------|
| æ‰‹å‹•ãƒ‡ãƒ—ãƒ­ã‚¤ | è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ |
| å€‹åˆ¥ç®¡ç† | çµ±åˆç®¡ç† |
| å®Ÿé¨“ä¸­å¿ƒ | é‹ç”¨ä¸­å¿ƒ |
| ãƒ¢ãƒ‡ãƒ«å˜ä½“ | ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ |

### MLOpsã®æ§‹æˆè¦ç´ 
1. **å®Ÿé¨“ç®¡ç†**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
2. **ãƒ¢ãƒ‡ãƒ«ç®¡ç†**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãƒ»ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
3. **CI/CD**: ç¶™ç¶šçš„çµ±åˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
4. **ç›£è¦–**: æ€§èƒ½ãƒ»ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥
5. **å†å­¦ç¿’**: è‡ªå‹•ãƒ»åŠè‡ªå‹•æ›´æ–°

## å®Ÿé¨“ç®¡ç†ãƒ»è¿½è·¡

### MLflow ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†
```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# å®Ÿé¨“ã®é–‹å§‹
mlflow.set_experiment("customer_churn_prediction")

def train_model(n_estimators, max_depth, min_samples_split):
    with mlflow.start_run():
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
        mlflow.log_param("n_estimators", n_estimators)
        mlflow.log_param("max_depth", max_depth)
        mlflow.log_param("min_samples_split", min_samples_split)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            random_state=42
        )
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("f1_score", f1)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        mlflow.sklearn.log_model(model, "model")
        
        # è¿½åŠ ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': model.feature_importances_
        })
        feature_importance.to_csv("feature_importance.csv")
        mlflow.log_artifact("feature_importance.csv")
        
        return model, accuracy

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
param_grid = [
    {"n_estimators": 100, "max_depth": 10, "min_samples_split": 2},
    {"n_estimators": 200, "max_depth": 15, "min_samples_split": 5},
    {"n_estimators": 300, "max_depth": 20, "min_samples_split": 10}
]

best_model = None
best_accuracy = 0

for params in param_grid:
    model, accuracy = train_model(**params)
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = model

print(f"Best accuracy: {best_accuracy}")
```

### Weights & Biases (wandb) æ´»ç”¨
```python
import wandb

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
wandb.init(project="deep_learning_experiment", config={
    "learning_rate": 0.001,
    "epochs": 100,
    "batch_size": 32,
    "architecture": "CNN"
})

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(wandb.config.epochs):
    train_loss = train_epoch()
    val_loss, val_accuracy = validate()
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "val_loss": val_loss,
        "val_accuracy": val_accuracy
    })
    
    # ç”»åƒãƒ»ã‚°ãƒ©ãƒ•ã®è¨˜éŒ²
    if epoch % 10 == 0:
        wandb.log({"predictions": wandb.Image(prediction_plot)})
        wandb.log({"confusion_matrix": wandb.plot.confusion_matrix(
            probs=None, y_true=y_true, preds=y_pred, class_names=class_names
        )})

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
wandb.save("model.h5")
```

## ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

### MLflow Model Registry
```python
# ãƒ¢ãƒ‡ãƒ«ç™»éŒ²
model_name = "customer_churn_model"

# æœ€æ–°ã®å®Ÿé¨“ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ç™»éŒ²
latest_run = mlflow.search_runs(experiment_ids=["1"]).iloc[0]
model_uri = f"runs:/{latest_run.run_id}/model"

model_version = mlflow.register_model(
    model_uri=model_uri,
    name=model_name
)

# ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†
client = mlflow.tracking.MlflowClient()

# Stagingç’°å¢ƒã¸ç§»è¡Œ
client.transition_model_version_stage(
    name=model_name,
    version=model_version.version,
    stage="Staging"
)

# Productionç’°å¢ƒã¸ç§»è¡Œï¼ˆæ‰¿èªå¾Œï¼‰
client.transition_model_version_stage(
    name=model_name,
    version=model_version.version,
    stage="Production"
)

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
def load_production_model(model_name):
    model_uri = f"models:/{model_name}/Production"
    model = mlflow.sklearn.load_model(model_uri)
    return model
```

### DVC (Data Version Control)
```bash
# ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
dvc init
dvc add data/train.csv
git add data/train.csv.dvc .gitignore
git commit -m "Add training data"

# ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
dvc add models/model.pkl
git add models/model.pkl.dvc
git commit -m "Add trained model v1.0"

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
# dvc.yaml
stages:
  prepare:
    cmd: python src/prepare.py
    deps:
    - src/prepare.py
    - data/raw
    outs:
    - data/processed
  
  train:
    cmd: python src/train.py
    deps:
    - src/train.py
    - data/processed
    params:
    - train.learning_rate
    - train.epochs
    outs:
    - models/model.pkl
    metrics:
    - metrics.json

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
dvc repro
```

## ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### REST API ãƒ‡ãƒ—ãƒ­ã‚¤ (FastAPI)
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np
import pandas as pd
from typing import List

app = FastAPI(title="ML Model API", version="1.0.0")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
model = joblib.load("models/production_model.pkl")
scaler = joblib.load("models/scaler.pkl")

class PredictionInput(BaseModel):
    features: List[float]
    
class PredictionOutput(BaseModel):
    prediction: float
    probability: List[float]
    model_version: str

@app.post("/predict", response_model=PredictionOutput)
async def predict(input_data: PredictionInput):
    try:
        # å‰å‡¦ç†
        features = np.array(input_data.features).reshape(1, -1)
        features_scaled = scaler.transform(features)
        
        # äºˆæ¸¬
        prediction = model.predict(features_scaled)[0]
        probability = model.predict_proba(features_scaled)[0].tolist()
        
        return PredictionOutput(
            prediction=float(prediction),
            probability=probability,
            model_version="1.0.0"
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": model is not None}

# ãƒãƒƒãƒäºˆæ¸¬ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/predict_batch")
async def predict_batch(input_data: List[PredictionInput]):
    results = []
    for item in input_data:
        pred_result = await predict(item)
        results.append(pred_result)
    return results
```

### DockeråŒ–
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  ml-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/models/model.pkl
    volumes:
      - ./models:/app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Kubernetes ãƒ‡ãƒ—ãƒ­ã‚¤
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model-api
  template:
    metadata:
      labels:
        app: ml-model-api
    spec:
      containers:
      - name: ml-api
        image: ml-model:latest
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_PATH
          value: "/app/models/model.pkl"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: ml-model-service
spec:
  selector:
    app: ml-model-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

## CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### GitHub Actions
```yaml
# .github/workflows/ml-pipeline.yml
name: ML Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest
    
    - name: Run tests
      run: |
        pytest tests/
    
    - name: Data validation
      run: |
        python src/validate_data.py
    
    - name: Model training
      run: |
        python src/train.py
        
    - name: Model validation
      run: |
        python src/validate_model.py

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Build Docker image
      run: |
        docker build -t ml-model:${{ github.sha }} .
    
    - name: Deploy to staging
      run: |
        # Stagingç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤
        kubectl apply -f k8s/staging/
    
    - name: Integration tests
      run: |
        python tests/integration_tests.py
    
    - name: Deploy to production
      run: |
        # Productionç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤
        kubectl apply -f k8s/production/
```

### Jenkins Pipeline
```groovy
pipeline {
    agent any
    
    environment {
        DOCKER_REGISTRY = 'your-registry.com'
        MODEL_NAME = 'customer-churn-model'
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Install Dependencies') {
            steps {
                sh 'pip install -r requirements.txt'
            }
        }
        
        stage('Data Validation') {
            steps {
                sh 'python src/validate_data.py'
            }
        }
        
        stage('Model Training') {
            steps {
                sh 'python src/train.py'
                archiveArtifacts artifacts: 'models/**/*', fingerprint: true
            }
        }
        
        stage('Model Testing') {
            steps {
                sh 'python src/test_model.py'
                publishTestResults testResultsPattern: 'test-results.xml'
            }
        }
        
        stage('Build Docker Image') {
            steps {
                script {
                    def image = docker.build("${DOCKER_REGISTRY}/${MODEL_NAME}:${BUILD_NUMBER}")
                    docker.withRegistry("https://${DOCKER_REGISTRY}") {
                        image.push()
                        image.push("latest")
                    }
                }
            }
        }
        
        stage('Deploy to Staging') {
            steps {
                sh 'kubectl apply -f k8s/staging/'
                sh 'kubectl rollout status deployment/ml-model-api -n staging'
            }
        }
        
        stage('Integration Tests') {
            steps {
                sh 'python tests/integration_tests.py'
            }
        }
        
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                input message: 'Deploy to production?', ok: 'Deploy'
                sh 'kubectl apply -f k8s/production/'
                sh 'kubectl rollout status deployment/ml-model-api -n production'
            }
        }
    }
    
    post {
        always {
            cleanWs()
        }
        failure {
            emailext (
                subject: "Build Failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}",
                body: "Build failed. Check console output at ${env.BUILD_URL}",
                to: "${env.CHANGE_AUTHOR_EMAIL}"
            )
        }
    }
}
```

## ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ

### ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–
```python
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge
import time

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
prediction_counter = Counter('ml_predictions_total', 'Total predictions made')
prediction_latency = Histogram('ml_prediction_duration_seconds', 'Prediction latency')
model_accuracy = Gauge('ml_model_accuracy', 'Current model accuracy')
data_drift_score = Gauge('ml_data_drift_score', 'Data drift score')

class ModelMonitor:
    def __init__(self, model, reference_data):
        self.model = model
        self.reference_data = reference_data
        self.prediction_buffer = []
        
    def predict_with_monitoring(self, X):
        start_time = time.time()
        
        try:
            # äºˆæ¸¬å®Ÿè¡Œ
            predictions = self.model.predict(X)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            prediction_counter.inc()
            prediction_latency.observe(time.time() - start_time)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥
            drift_score = self.detect_data_drift(X)
            data_drift_score.set(drift_score)
            
            # äºˆæ¸¬çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
            self.prediction_buffer.extend(predictions.tolist())
            
            return predictions
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            error_counter.inc()
            raise e
    
    def detect_data_drift(self, X):
        from scipy import stats
        
        drift_scores = []
        for i, feature in enumerate(X.T):
            # Kolmogorov-Smirnov test
            statistic, p_value = stats.ks_2samp(
                self.reference_data[:, i], 
                feature
            )
            drift_scores.append(statistic)
        
        return np.mean(drift_scores)
    
    def update_accuracy(self, y_true, y_pred):
        accuracy = np.mean(y_true == y_pred)
        model_accuracy.set(accuracy)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶
        if accuracy < 0.8:
            self.send_alert(f"Model accuracy dropped to {accuracy:.2f}")
    
    def send_alert(self, message):
        # Slacké€šçŸ¥
        import requests
        webhook_url = "YOUR_SLACK_WEBHOOK_URL"
        payload = {"text": f"ğŸš¨ ML Model Alert: {message}"}
        requests.post(webhook_url, json=payload)
```

### Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
```yaml
# grafana-dashboard.json (æŠœç²‹)
{
  "dashboard": {
    "title": "ML Model Monitoring",
    "panels": [
      {
        "title": "Prediction Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ml_predictions_total[5m])",
            "legendFormat": "Predictions/sec"
          }
        ]
      },
      {
        "title": "Prediction Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, ml_prediction_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Model Accuracy",
        "type": "singlestat",
        "targets": [
          {
            "expr": "ml_model_accuracy",
            "legendFormat": "Accuracy"
          }
        ]
      }
    ]
  }
}
```

## A/Bãƒ†ã‚¹ãƒˆãƒ»ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤

### A/Bãƒ†ã‚¹ãƒˆå®Ÿè£…
```python
class ABTestFramework:
    def __init__(self, model_a, model_b, traffic_split=0.5):
        self.model_a = model_a
        self.model_b = model_b
        self.traffic_split = traffic_split
        self.results_a = []
        self.results_b = []
    
    def predict(self, X, user_id):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«é¸æŠ
        use_model_b = hash(user_id) % 100 < (self.traffic_split * 100)
        
        if use_model_b:
            prediction = self.model_b.predict(X)
            self.results_b.append({
                'user_id': user_id,
                'prediction': prediction,
                'timestamp': time.time()
            })
            return prediction, 'model_b'
        else:
            prediction = self.model_a.predict(X)
            self.results_a.append({
                'user_id': user_id,
                'prediction': prediction,
                'timestamp': time.time()
            })
            return prediction, 'model_a'
    
    def analyze_results(self):
        from scipy import stats
        
        # çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š
        metric_a = [r['metric'] for r in self.results_a]
        metric_b = [r['metric'] for r in self.results_b]
        
        t_stat, p_value = stats.ttest_ind(metric_a, metric_b)
        
        return {
            'model_a_mean': np.mean(metric_a),
            'model_b_mean': np.mean(metric_b),
            'p_value': p_value,
            'significant': p_value < 0.05
        }
```

### Istio ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤
```yaml
# canary-deployment.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: ml-model-vs
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: ml-model-service
        subset: v2
  - route:
    - destination:
        host: ml-model-service
        subset: v1
      weight: 90
    - destination:
        host: ml-model-service
        subset: v2
      weight: 10

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: ml-model-dr
spec:
  host: ml-model-service
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

## ã¾ã¨ã‚

MLOpsã¯æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨åŒ–ã«ä¸å¯æ¬ ãªæŠ€è¡“ä½“ç³»ã§ã™ã€‚å®Ÿé¨“ç®¡ç†ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã€CI/CDã€ç›£è¦–ã€A/Bãƒ†ã‚¹ãƒˆã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã€å®‰å®šã—ãŸMLã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚

æ¬¡å›ã¯ã€ŒAI ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ã«ã¤ã„ã¦å­¦ç¿’ã—ã€AIã‚·ã‚¹ãƒ†ãƒ ã®å®‰å…¨æ€§ç¢ºä¿ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚